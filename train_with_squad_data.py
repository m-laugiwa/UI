# -*- coding: utf-8 -*-
"""Train_with_SQuAD_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_vBxLHVde5t-o8BYsWwL3odaIfmh7xXC
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Ambil Data SQuAD"""

# import os
# import requests

# # Specify the target directory to store the Squad dataset
# target_dir = '/content/drive/MyDrive/Prodject/QA_IR/SQuAD_Data'

# # Create the directory if it doesn't exist
# if not os.path.exists(target_dir):
#     os.makedirs(target_dir)

# # List of files to download
# files_to_download = ['train-v2.0.json', 'dev-v2.0.json']

# # Download each file
# for file in files_to_download:
#     url = f'https://rajpurkar.github.io/SQuAD-explorer/dataset/{file}'

#     # Make the request to download data over HTTP
#     res = requests.get(url)

#     # Write to file in the target directory
#     with open(os.path.join(target_dir, file), 'wb') as f:
#         for chunk in res.iter_content(chunk_size=128):
#             f.write(chunk)

# print("SQuAD dataset downloaded successfully to:", target_dir)

"""# Load and Data Preparation"""

import json
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')

def read_squad(path):
    with open(path, 'r', encoding='utf-8') as f:
        dt = json.load(f)

    contexts = []
    answers = []
    questions = []

    for data_entry in dt["data"]:
        paragraphs = data_entry["paragraphs"]
        for paragraph in paragraphs:
            context = paragraph['context']
            cleaned_context = ' '.join(word_tokenize(context.lower()))

            for qa in paragraph['qas']:
                question = qa['question']
                cleaned_question = ' '.join(word_tokenize(question.lower()))

                if 'answers' in qa and len(qa['answers']) > 0:
                    answer = qa['answers'][0]  # Assuming there is only one answer per question
                    cleaned_answer = ' '.join(word_tokenize(answer['text'].lower()))
                    answer_start = answer.get('answer_start', -1)
                    answer_end = answer.get('answer_end', -1)
                    contexts.append(cleaned_context)
                    questions.append(cleaned_question)
                    answers.append({
                        'text': cleaned_answer,
                        'answer_start': answer_start,
                        'answer_end': answer_end
                    })
                else:
                    contexts.append(cleaned_context)
                    questions.append(cleaned_question)
                    answers.append({
                        'text': '',
                        'answer_start': -1,
                        'answer_end': -1
                    })

    return contexts, questions, answers

# Usage example for both training and validation data
train_data_path = "/content/drive/MyDrive/Prodject/QA_IR/SQuAD_Data/train-v2.0.json"
val_data_path = "/content/drive/MyDrive/Prodject/QA_IR/SQuAD_Data/dev-v2.0.json"

train_contexts, train_questions, train_answers = read_squad(train_data_path)
val_contexts, val_questions, val_answers = read_squad(val_data_path)

# Checking the cleaned context for the first example in training and validation data
print("Training contexts:", train_contexts[:1])
print("Validation contexts:", val_contexts[:1])

train_contexts[:1]

train_answers[:1]

train_questions[:1]

"""## Recreate Answer end after Cleaning"""

# Iterate through answers and contexts to find answer start indices
for answer, context in zip(train_answers, train_contexts):
    answer_start = context.find(answer['text'])
    answer_end = answer_start + len(answer['text']) - 1

    # Update answer start and end indices in the answers list
    answer['answer_start'] = answer_start
    answer['answer_end'] = answer_end

train_contexts[:1]

train_questions[:1]

train_answers[:1]

! pip install transformers

"""# Indobert Base

Try Indobert-base-p2 by Willie
"""

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("indolem/indobert-base-uncased")
model = AutoModel.from_pretrained("indolem/indobert-base-uncased")

print(model)

from transformers import AutoTokenizer

# Load the tokenizer associated with your pre-trained model
tokenizer = AutoTokenizer.from_pretrained("indolem/indobert-base-uncased")

# Tokenize and encode training data
train_encoding = tokenizer(train_contexts, train_questions, truncation=True, padding=True, max_length=512)

# Tokenize and encode validation data
val_encoding = tokenizer(val_contexts, val_questions, truncation=True, padding=True, max_length=512)

## add token position

def add_token_positions(encodings, answers, tokenizer):
    start_positions = []
    end_positions = []

    for i in range(len(answers)):
        # Get answer positions
        answer_start = answers[i]['answer_start']
        answer_end = answers[i]['answer_end']

        # Tokenize the answer
        answer_tokens = tokenizer(answers[i]['text'], add_special_tokens=False)['input_ids']

        # Find the positions of the start and end tokens in the tokenized input
        if answer_start >= 0 and answer_end >= 0:
            start_positions.append(encodings.char_to_token(i, answer_start))
            end_positions.append(encodings.char_to_token(i, answer_end))
        else:
            # Handle questions without answers
            start_positions.append(None)
            end_positions.append(None)

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})


# Apply the function to your data
add_token_positions(train_encoding, train_answers, tokenizer)
add_token_positions(val_encoding, val_answers, tokenizer)

train_encoding.keys()

train_encoding['start_positions'][:10]

train_encoding['end_positions'][:10]

val_encoding['end_positions'][:10]

# Check for None values in train_encodings
for key, value in train_encoding.items():
    if value is None:
        print(f"train_encodings['{key}'] contains a None value.")

# Check for None values in val_encodings
for key, value in val_encoding.items():
    if value is None:
        print(f"val_encodings['{key}'] contains a None value.")

train_answers[:10]

"""## Fine Tunning"""

import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) if val[idx] is not None else torch.tensor(0) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings.input_ids)

# build datasets for both our training and validation sets
train_dataset = SquadDataset(train_encoding)
val_dataset = SquadDataset(val_encoding)

"""## Try with 10 Epoch"""

from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch

# Load tokenizer and question answering model
tokenizer = AutoTokenizer.from_pretrained("indolem/indobert-base-uncased")
model = AutoModelForQuestionAnswering.from_pretrained("indolem/indobert-base-uncased")

# Setup GPU/CPU
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)  # Move model to the appropriate device

# Initialize optimizer
optim = AdamW(model.parameters(), lr=5e-5)

# initialize data loader for training data
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

for epoch in range(10):
    # set model to train mode
    model.train()
    # setup loop (we use tqdm for the progress bar)
    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all the tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)
        # train model on batch and return outputs (incl. loss)
        outputs = model(input_ids, attention_mask=attention_mask,
                        start_positions=start_positions,
                        end_positions=end_positions)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())

"""### Save The model of 10 epoch"""

# Save the trained model and tokenizer to the specified directory
import os
output_model_dir = '/content/drive/MyDrive/Prodject/QA_IR/Model_After_SQuAD_Data/Indobert_10e_af_SQuAD'
model.save_pretrained(output_model_dir)
tokenizer.save_pretrained(output_model_dir)
torch.save(optim.state_dict(), os.path.join(output_model_dir, 'optimizer.pt'))

print("Trained model, tokenizer, and optimizer state saved successfully.")

"""### Evaluating 10 epoch"""

import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from sklearn.metrics import f1_score
from transformers import AdamW

# Load the fine-tuned model and tokenizer
model_path = '/content/drive/MyDrive/Prodject/QA_IR/Model_After_SQuAD_Data/Indobert_10e_af_SQuAD'
tokenizer1 = AutoTokenizer.from_pretrained(model_path)
model1 = AutoModelForQuestionAnswering.from_pretrained(
    model_path,
    from_tf=False  # Set this to True
)

# Extract answer texts from val_answers
answer_texts = [item['text'] for item in val_answers]

# Define your evaluation data (contexts, questions, and answers)
val_contexts = val_contexts  # List of evaluation contexts
val_questions = val_questions  # List of evaluation questions
val_answers = answer_texts  # List of evaluation answers (ground truth)

# Tokenize and encode the evaluation data with a specified maximum length
eval_encoding = tokenizer1(
    val_contexts,
    val_questions,
    truncation=True,
    padding=True,
    max_length=512  # Specify the maximum length
)

# Create a PyTorch dataset for evaluation
from torch.utils.data import Dataset

class EvalDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

eval_dataset = EvalDataset(eval_encoding)

# Create a DataLoader for evaluation
from torch.utils.data import DataLoader
eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=False)

# Initialize lists to store predicted answers
predicted_answers = []

# Set the model to evaluation mode
model1.eval()

# Iterate through the evaluation data and make predictions
with torch.no_grad():
    for batch in eval_loader:
        input_ids = batch['input_ids'].to(model1.device)
        attention_mask = batch['attention_mask'].to(model1.device)

        # Use the model to predict answers
        outputs = model1(input_ids, attention_mask=attention_mask)

        # Extract predicted answer indices from the model's output
        start_logits, end_logits = outputs.start_logits, outputs.end_logits
        start_indices = torch.argmax(start_logits, dim=1)
        end_indices = torch.argmax(end_logits, dim=1)

        # Convert token indices to text
        batch_answers = []
        for i in range(input_ids.size(0)):
            start_index = start_indices[i].item()
            end_index = end_indices[i].item()
            answer_tokens = input_ids[i][start_index : end_index + 1]
            answer_text = tokenizer1.decode(answer_tokens, skip_special_tokens=True)
            batch_answers.append(answer_text)

        predicted_answers.extend(batch_answers)

# Calculate the F1 score
f1 = f1_score(val_answers, predicted_answers, average='micro')

print("F1 Score:", f1)

"""# Roberta Base"""

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline

model_name = "deepset/roberta-base-squad2"

# a) Get predictions
nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)
QA_input = {
    'question': 'Why is model conversion important?',
    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'
}
res = nlp(QA_input)

# b) Load model & tokenizer
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

print(model)

from transformers import AutoTokenizer

# Tokenize and encode training data
train_encoding = tokenizer(train_contexts, train_questions, truncation=True, padding=True, max_length=512)

# Tokenize and encode validation data
val_encoding = tokenizer(val_contexts, val_questions, truncation=True, padding=True, max_length=512)

## add token position

def add_token_positions(encodings, answers, tokenizer):
    start_positions = []
    end_positions = []

    for i in range(len(answers)):
        # Get answer positions
        answer_start = answers[i]['answer_start']
        answer_end = answers[i]['answer_end']

        # Tokenize the answer
        answer_tokens = tokenizer(answers[i]['text'], add_special_tokens=False)['input_ids']

        # Find the positions of the start and end tokens in the tokenized input
        if answer_start >= 0 and answer_end >= 0:
            start_positions.append(encodings.char_to_token(i, answer_start))
            end_positions.append(encodings.char_to_token(i, answer_end))
        else:
            # Handle questions without answers
            start_positions.append(None)
            end_positions.append(None)

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})


# Apply the function to your data
add_token_positions(train_encoding, train_answers, tokenizer)
add_token_positions(val_encoding, val_answers, tokenizer)

train_encoding.keys()

train_encoding['start_positions'][:10]

train_encoding['end_positions'][:10]

val_encoding['end_positions'][:10]

# Check for None values in train_encodings
for key, value in train_encoding.items():
    if value is None:
        print(f"train_encodings['{key}'] contains a None value.")

# Check for None values in val_encodings
for key, value in val_encoding.items():
    if value is None:
        print(f"val_encodings['{key}'] contains a None value.")

train_answers[:10]

"""## Fine Tunning"""

import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) if val[idx] is not None else torch.tensor(0) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings.input_ids)

# build datasets for both our training and validation sets
train_dataset = SquadDataset(train_encoding)
val_dataset = SquadDataset(val_encoding)

"""## Try with 10 Epoch"""

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from transformers import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm

# setup GPU/CPU
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
# move model over to detected device
model.to(device)
# activate training mode of model
model.train()
# initialize adam optimizer with weight decay (reduces chance of overfitting)
optim = AdamW(model.parameters(), lr=5e-5)

# initialize data loader for training data
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

for epoch in range(10):
    # set model to train mode
    model.train()
    # setup loop (we use tqdm for the progress bar)
    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all the tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)
        # train model on batch and return outputs (incl. loss)
        outputs = model(input_ids, attention_mask=attention_mask,
                        start_positions=start_positions,
                        end_positions=end_positions)
        # extract loss
        loss = outputs[0]
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())

"""### Save the model of 10 epoch"""

# Save the trained model and tokenizer to the specified directory
import os
output_model_dir = '/content/drive/MyDrive/Prodject/QA_IR/Model_After_SQuAD_Data/Roberta_10e_af_SQuAD'
model.save_pretrained(output_model_dir)
tokenizer.save_pretrained(output_model_dir)
torch.save(optim.state_dict(), os.path.join(output_model_dir, 'optimizer.pt'))

print("Trained model, tokenizer, and optimizer state saved successfully.")

"""## Evaluating 10 epoch"""

import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from sklearn.metrics import f1_score
from transformers import AdamW

# Load the fine-tuned model and tokenizer
model_path = '/content/drive/MyDrive/Prodject/QA_IR/Model_After_SQuAD_Data/Roberta_10e_af_SQuAD'
tokenizer1 = AutoTokenizer.from_pretrained(model_path)
model1 = AutoModelForQuestionAnswering.from_pretrained(
    model_path,
    from_tf=False  # Set this to True
)

# Extract answer texts from val_answers
answer_texts = [item['text'] for item in val_answers]

# Define your evaluation data (contexts, questions, and answers)
val_contexts = val_contexts  # List of evaluation contexts
val_questions = val_questions  # List of evaluation questions
val_answers = answer_texts  # List of evaluation answers (ground truth)

# Tokenize and encode the evaluation data with a specified maximum length
eval_encoding = tokenizer1(
    val_contexts,
    val_questions,
    truncation=True,
    padding=True,
    max_length=512  # Specify the maximum length
)

# Create a PyTorch dataset for evaluation
from torch.utils.data import Dataset

class EvalDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

eval_dataset = EvalDataset(eval_encoding)

# Create a DataLoader for evaluation
from torch.utils.data import DataLoader
eval_loader = DataLoader(eval_dataset, batch_size=8, shuffle=False)

# Initialize lists to store predicted answers
predicted_answers = []

# Set the model to evaluation mode
model1.eval()

# Iterate through the evaluation data and make predictions
with torch.no_grad():
    for batch in eval_loader:
        input_ids = batch['input_ids'].to(model1.device)
        attention_mask = batch['attention_mask'].to(model1.device)

        # Use the model to predict answers
        outputs = model1(input_ids, attention_mask=attention_mask)

        # Extract predicted answer indices from the model's output
        start_logits, end_logits = outputs.start_logits, outputs.end_logits
        start_indices = torch.argmax(start_logits, dim=1)
        end_indices = torch.argmax(end_logits, dim=1)

        # Convert token indices to text
        batch_answers = []
        for i in range(input_ids.size(0)):
            start_index = start_indices[i].item()
            end_index = end_indices[i].item()
            answer_tokens = input_ids[i][start_index : end_index + 1]
            answer_text = tokenizer1.decode(answer_tokens, skip_special_tokens=True)
            batch_answers.append(answer_text)

        predicted_answers.extend(batch_answers)

# Calculate the F1 score
f1 = f1_score(val_answers, predicted_answers, average='micro')

print("F1 Score:", f1)

"""# XLM Roberta Base"""

from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained('Palak/xlm-roberta-base_squad')
model = AutoModelForMaskedLM.from_pretrained("Palak/xlm-roberta-base_squad")

# prepare input
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')

# forward pass
output = model(**encoded_input)

print(model)

from transformers import AutoTokenizer

# Tokenize and encode training data
train_encoding = tokenizer(train_contexts, train_questions, truncation=True, padding=True, max_length=512)

# Tokenize and encode validation data
val_encoding = tokenizer(val_contexts, val_questions, truncation=True, padding=True, max_length=512)

## add token position

def add_token_positions(encodings, answers, tokenizer):
    start_positions = []
    end_positions = []

    for i in range(len(answers)):
        # Get answer positions
        answer_start = answers[i]['answer_start']
        answer_end = answers[i]['answer_end']

        # Tokenize the answer
        answer_tokens = tokenizer(answers[i]['text'], add_special_tokens=False)['input_ids']

        # Find the positions of the start and end tokens in the tokenized input
        if answer_start >= 0 and answer_end >= 0:
            start_positions.append(encodings.char_to_token(i, answer_start))
            end_positions.append(encodings.char_to_token(i, answer_end))
        else:
            # Handle questions without answers
            start_positions.append(None)
            end_positions.append(None)

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})


# Apply the function to your data
add_token_positions(train_encoding, train_answers, tokenizer)
add_token_positions(val_encoding, val_answers, tokenizer)

train_encoding.keys()

train_encoding['start_positions'][:10]

train_encoding['end_positions'][:10]

val_encoding['end_positions'][:10]

# Check for None values in train_encodings
for key, value in train_encoding.items():
    if value is None:
        print(f"train_encodings['{key}'] contains a None value.")

# Check for None values in val_encodings
for key, value in val_encoding.items():
    if value is None:
        print(f"val_encodings['{key}'] contains a None value.")

train_answers[:10]

"""## Fine Tuning"""

import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) if val[idx] is not None else torch.tensor(0) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings.input_ids)

# build datasets for both our training and validation sets
train_dataset = SquadDataset(train_encoding)
val_dataset = SquadDataset(val_encoding)

"""## Try with 10 Epoch"""

from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch

# Load your fine-tuned model and tokenizer
model_path = "Palak/xlm-roberta-base_squad"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)
model.to(device)  # Move model to the appropriate device

# Initialize optimizer
optim = AdamW(model.parameters(), lr=5e-5)

# initialize data loader for training data
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

for epoch in range(10):
    # set model to train mode
    model.train()
    # setup loop (we use tqdm for the progress bar)
    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all the tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)
        # train model on batch and return outputs (incl. loss)
        outputs = model(input_ids, attention_mask=attention_mask,
                        start_positions=start_positions,
                        end_positions=end_positions)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())

"""### Save the model of 10 epoch"""

# Save the trained model and tokenizer to the specified directory
import os
output_model_dir = '/content/drive/MyDrive/Prodject/QA_IR/Model_After_SQuAD_Data/Roberta_10e_af_SQuAD'
model.save_pretrained(output_model_dir)
tokenizer.save_pretrained(output_model_dir)
torch.save(optim.state_dict(), os.path.join(output_model_dir, 'optimizer.pt'))

print("Trained model, tokenizer, and optimizer state saved successfully.")